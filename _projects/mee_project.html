---
layout: page
title: MEE Project
permalink: /projects/mee_project/
description: Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation
img: /assets/img/mee_project/enc.jpg
importance: 1
category: work
---

<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="https://rail-vln.github.io/projects/mee_project/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <!-- <meta property="og:image" content="/assets/img/banner_model.png" /> -->
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Multimodal Evolutionary Encoder, Vision-Language Vision-Language in Continuous Environments" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>MEE-Project</title>
    <link rel="icon" type="image/x-icon" href="/assets/img/mee_project/favicon.ico" />
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

    <link rel="stylesheet" href="/assets/css/bulma.min.css" />
    <link rel="stylesheet" href="/assets/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="/assets/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="/assets/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="/assets/css/mee_project_index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="/assets/js/fontawesome.all.min.js"></script>
    <script src="/assets/js/bulma-carousel.min.js"></script>
    <script src="/assets/js/bulma-slider.min.js"></script>
    <script src="/assets/js/mee_project_index.js"></script>
  </head>
  <body>
    <!-- Scroll to Top Button -->
    <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
      <i class="fas fa-chevron-up"></i>
    </button>

    <!-- More Works Dropdown -->
    <div class="more-works-container">
      <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
        <i class="fas fa-flask"></i>
        More Works
        <i class="fas fa-chevron-down dropdown-arrow"></i>
      </button>
      <div class="more-works-dropdown" id="moreWorksDropdown">
        <div class="dropdown-header">
          <h4>More Works from Our Lab</h4>
          <button class="close-btn" onclick="toggleMoreWorks()">
            <i class="fas fa-times"></i>
          </button>
        </div>
        <div class="works-list">
          <!-- TODO: Replace with your lab's related works -->
          <a href="https://ieeexplore.ieee.org/document/10288539" class="work-item" target="_blank">
            <div class="work-info">
              <h5>Learning Depth Representation From RGB-D Videos by Time-Aware Contrastive Pre-Training</h5>
              <!-- <p>Brief description of the work and its main contribution.</p> -->
              <span class="work-venue">IEEE Transactions on Circuits and Systems for Video Technology 2024</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
          <!-- TODO: Add more related works or remove extra items -->
          <a href="https://ieeexplore.ieee.org/document/11115085" class="work-item" target="_blank">
            <div class="work-info">
              <h5>NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization</h5>
              <!-- <p>Brief description of the work and its main contribution.</p> -->
              <span class="work-venue">IEEE Transactions on Circuits and Systems for Video Technology (Early Access)</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
          <a href="https://ieeexplore.ieee.org/document/10658053" class="work-item" target="_blank">
            <div class="work-info">
              <h5>Vision-and-Language Navigation via Causal Learning</h5>
              <!-- <p>Brief description of the work and its main contribution.</p> -->
              <span class="work-venue">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
        </div>
      </div>
    </div>

    <!-- <main id="main-content"> -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation</h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block"> <a href="https://scholar.google.com/citations?user=LtncSTwAAAAJ" target="_blank">Zongtao He</a>, </span>
                <span class="author-block"> <a href="https://scholar.google.com/citations?user=AW2gZ8cAAAAJ" target="_blank">Liuyi Wang</a>, </span>
                <span class="author-block"> <a href="https://ieeexplore.ieee.org/author/929490101091915" target="_blank">Lu Chen</a>, </span>
                <span class="author-block"> <a href="https://ieeexplore.ieee.org/author/37086349133" target="_blank">Shu Li</a>, </span>
                <span class="author-block"> <a href="https://ieeexplore.ieee.org/author/37089298351" target="_blank">Qingqing Yan</a>, </span>
                <span class="author-block"> <a href="https://scholar.google.com/citations?user=O55rDMQAAAAJ" target="_blank">Chengju Liu</a>, </span>
                <span class="author-block">
                  <a href="https://ieeexplore.ieee.org/author/37276133600" target="_blank">Qijun Chen</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">Department of Control Science and Engineering, Tongji University</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="https://ieeexplore.ieee.org/document/10802484" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper (IROS 2024)</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/RavenKiller/MEE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <!-- TODO: Replace with your teaser video -->
          <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
            <source src="https://github.com/user-attachments/assets/5edc8cc0-3a57-4e05-b785-f45ad1302e52" type="video/mp4" />
          </video>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Can multimodal encoder evolve when facing increasingly tough circumstances? Our work investigates this possibility in the context of
                continuous vision-language navigation (continuous VLN), which aims to navigate robots under linguistic supervision and visual
                feedback. We propose a multimodal evolutionary encoder (MEE) comprising a unified multimodal encoder architecture and an evolutionary
                pre-training strategy. The unified multimodal encoder unifies rich modalities, including depth and sub-instruction, to enhance the
                solid understanding of environments and tasks. It also effectively utilizes monocular observation, reducing the reliance on panoramic
                vision. The evolutionary pre-training strategy exposes the encoder to increasingly unfamiliar data domains and difficult objectives.
                The multi-stage adaption helps the encoder establish robust inner- and inter-modality connections and improve its generalization to
                unfamiliar environments. To achieve such evolution, we collect a large-scale multi-stage dataset with specialized objectives,
                addressing the absence of suitable continuous VLN pre-training. Evaluation on VLN-CE demonstrates the superiority of MEE over other
                direct action-predicting methods. Furthermore, we deploy MEE in real scenes using self-developed service robots, showcasing its
                effectiveness and potential for real-world applications.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <section class="section hero is-small">
      <div class="hero-body">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Method</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="/assets/img/mee_project/all.jpg" alt="Overall procedure" />
              <br />
              <h2 class="subtitle has-text-centered">Overall procedure.</h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="/assets/img/mee_project/evo.jpg" alt="The evolutionary pre-training" />
              <br />
              <h2 class="subtitle has-text-centered">The evolutionary pre-training procedure and dataset.</h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="/assets/img/mee_project/enc.jpg" alt="The model architecture" />
              <br />
              <h2 class="subtitle has-text-centered">The model architecture.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Contributions -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">Main Contributions</h2>
          <div class="content">
            <ul>
              <li>The unified multimodal encoder enhances comprehensive perception with unified features while reduces the reliance on panoramas.</li>
              <li>
                The evolutionary pre-training enables the encoder to evolve better feature representations and generalization ability across multiple
                stages.
              </li>
              <li>Both simulated and real scene experiments validate the effectiveness of MEE. Code and datasets have been publicly released.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>
    <!-- End Contributions -->

    <!--BibTex citation -->
    <section class="section hero is-small" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@INPROCEEDINGS{10802484,
  author={He, Zongtao and Wang, Liuyi and Chen, Lu and Li, Shu and Yan, Qingqing and Liu, Chengju and Chen, Qijun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation}, 
  year={2024},
  volume={},
  number={},
  pages={1443-1450},
  keywords={Visualization;Costs;Codes;Navigation;Service robots;Linguistics;Feature extraction;Solids;Decoding;Intelligent robots},
  doi={10.1109/IROS58592.2024.10802484}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was
                adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. <br />
                This website is licensed under a
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
