---
---

@string{aps = {American Physical Society,}}

@ARTICLE{10288539,
  author={He, Zongtao and Wang, Liuyi and Dang, Ronghao and Li, Shu and Yan, Qingqing and Liu, Chengju and Chen, Qijun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Learning Depth Representation From RGB-D Videos by Time-Aware Contrastive Pre-Training}, 
  year={2024},
  volume={34},
  number={6},
  pages={4143-4158},
  abstract={Existing end-to-end depth representation in embodied AI is often task-specific and lacks the benefits of emerging pre-training paradigm due to limited datasets and training techniques for RGB-D videos. To address the challenge of obtaining robust and generalized depth representation for embodied AI, we introduce a unified RGB-D video dataset (UniRGBD) and a novel time-aware contrastive (TAC) pre-training approach. UniRGBD addresses the scarcity of large-scale depth pre-training datasets by providing a comprehensive collection of data from diverse sources in a unified format, enabling convenient data loading and accommodating various data domains. We also design an RGB-Depth alignment evaluation procedure and introduce a novel Near-K accuracy metric to assess the scene understanding capability of the depth encoder. Then, the TAC pre-training approach fills the gap in depth pre-training methods suitable for RGB-D videos by leveraging the intrinsic similarity between temporally proximate frames. TAC incorporates a soft label design that acts as valid label noise, enhancing the depth semantic extraction and promoting diverse and generalized knowledge acquisition. Furthermore, the adjustments in perspective between temporally proximate frames facilitate the extraction of invariant and comprehensive features, enhancing the robustness of the learned depth representation. Additionally, the inclusion of temporal information stabilizes training gradients and enables spatio-temporal depth perception. Comprehensive evaluation of RGB-Depth alignment demonstrates the superiority of our approach over state-of-the-art methods. We also conduct uncertainty analysis and a novel zero-shot experiment to validate the robustness and generalization of the TAC approach. Moreover, our TAC pre-training demonstrates significant performance improvements in various embodied AI tasks, providing compelling evidence of its efficacy across diverse domains.},
  keywords={Task analysis;Artificial intelligence;Videos;Training;Databases;Visualization;Feature extraction;Depth representation;pre-training methods;contrastive learning;embodied AI},
  doi={10.1109/TCSVT.2023.3326373},
  ISSN={1558-2205},
  month={June},
  code={https://github.com/RavenKiller/TAC},
  gs_userid={LtncSTwAAAAJ},
  gs_paperid={Se3iqnhoufwC},
  bibtex_show={true},
  abbr={TAC},
  preview={10288539_tac.jpg},
  selected={true}
  }

@Article{He2024_IAHWP,
  author={He†, Zongtao
  and Wang†, Naijia
  and Wang, Liuyi
  and Liu, Chengju
  and Chen, Qijun},
  title={Instruction-aligned hierarchical waypoint planner for vision-and-language navigation in continuous environments},
  journal={Pattern Analysis and Applications},
  year={2024},
  month={Oct},
  day={03},
  volume={27},
  number={4},
  pages={132},
  abstract={Developing agents to follow language instructions is a compelling yet challenging research topic. Recently, vision-and-language navigation in continuous environments has been proposed to explore the multi-modal pattern analysis and mapless navigation abilities of intelligent agents. However, current waypoint-based methods still have shortcomings, such as the coupled decision process and the possible shortest path-instruction misalignment. To address these challenges, we propose an instruction-aligned hierarchical waypoint planner (IA-HWP) that ensures fine-grained waypoint prediction and enhances instruction alignment. Our HWP architecture decouples waypoint planning into a coarse view selection phase and a refined waypoint location phase, effectively improving waypoint quality and enabling specialized training supervision for different phases. In terms of instruction-aligned model design, we introduce the global action-vision co-grounding and local text-vision co-grounding modules to explicitly improve the understanding of visual landmarks and actions, thereby enhancing the alignment between instructions and trajectories. In terms of instruction-aligned model optimization, we employ reference-waypoint-oriented supervision and direction-aware loss to optimize the model for enhanced instruction following and waypoint execution capabilities. Experiments on the standard benchmark demonstrate the effectiveness of our approach, with improved success rate compared to existing methods.},
  issn={1433-755X},
  doi={10.1007/s10044-024-01339-z},
  url={https://doi.org/10.1007/s10044-024-01339-z},
  annotation={† Contribute equally to this work},
  code={https://github.com/RavenKiller/IA-HWP},
  gs_userid={LtncSTwAAAAJ},
  gs_paperid={MXK_kJrjxJIC},
  bibtex_show={true},
  abbr={IA-HWP},
  preview={He2024_IAHWP.jpg}
}


@INPROCEEDINGS{10802484,
  author={He, Zongtao and Wang, Liuyi and Chen, Lu and Li, Shu and Yan, Qingqing and Liu, Chengju and Chen, Qijun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation}, 
  year={2024},
  volume={},
  number={},
  pages={1443-1450},
  abstract={Can multimodal encoder evolve when facing increasingly tough circumstances? Our work investigates this possibility in the context of continuous vision-language navigation (continuous VLN), which aims to navigate robots under linguistic supervision and visual feedback. We propose a multimodal evolutionary encoder (MEE) comprising a unified multimodal encoder architecture and an evolutionary pre-training strategy. The unified multimodal encoder unifies rich modalities, including depth and sub-instruction, to enhance the solid understanding of environments and tasks. It also effectively utilizes monocular observation, reducing the reliance on panoramic vision. The evolutionary pre-training strategy exposes the encoder to increasingly unfamiliar data domains and difficult objectives. The multi-stage adaption helps the encoder establish robust intra- and inter-modality connections and improve its generalization to unfamiliar environments. To achieve such evolution, we collect a large-scale multi-stage dataset with specialized objectives, addressing the absence of suitable continuous VLN pre-training. Evaluation on VLN-CE demonstrates the superiority of MEE over other direct action-predicting methods. Furthermore, we deploy MEE in real scenes using self-developed service robots, showcasing its effectiveness and potential for real-world applications. Our code and dataset are available at https://github.com/RavenKiller/MEE.},
  keywords={Visualization;Costs;Codes;Navigation;Service robots;Linguistics;Feature extraction;Solids;Decoding;Intelligent robots},
  doi={10.1109/IROS58592.2024.10802484},
  ISSN={2153-0866},
  month={Oct},
  code={https://github.com/RavenKiller/MEE},
  gs_userid={LtncSTwAAAAJ},
  gs_paperid={kNdYIx-mwKoC},
  bibtex_show={true},
  abbr={MEE},
  preview={10802484_mee.jpg}
}

@Article{wang2023pasts,
  title={PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation},
  author={Wang, Liuyi and Liu, Chengju and He, Zongtao and Li, Shu and Yan, Qingqing and Chen, Huiyi and Chen, Qijun},
  journal={Engineering Applications of Artificial Intelligence},
  year={2023},
  doi={https://doi.org/10.1016/j.engappai.2023.107487},
  code={https://github.com/CrystalSixone/PASTS},
  abbr={PASTS},
  preview={wang2023pasts.png},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={Y0pCki6q_DkC},
  bibtex_show={true},
  abstract={Vision-and-language navigation (VLN) is a crucial but challenging cross-modal navigation task. One powerful technique to enhance the generalization performance in VLN is the use of an independent speaker model to provide pseudo instructions for data augmentation. However, current speaker models based on Long-Short Term Memory (LSTM) lack the ability to attend to features relevant at different locations and time steps. To address this, we propose a novel progress-aware spatio-temporal transformer speaker (PASTS) model that uses the transformer as the core of the network. PASTS uses a spatio-temporal encoder to fuse panoramic representations and encode intermediate connections through steps. Besides, to avoid the misalignment problem that could result in incorrect supervision, a speaker progress monitor (SPM) is proposed to enable the model to estimate the progress of instruction generation and facilitate more fine-grained caption results. Additionally, a multifeature dropout (MFD) strategy is introduced to alleviate overfitting. The proposed PASTS is flexible to be combined with existing VLN models. The experimental results demonstrate that PASTS outperforms previous speaker models and successfully improves the performance of previous VLN models, achieving state-of-the-art performance on the standard Room-to-Room (R2R) dataset.}
}

@InProceedings{Wang_2024_GOAT,
    author    = {Wang, Liuyi and He, Zongtao and Dang, Ronghao and Shen, Mengjiao and Liu, Chengju and Chen, Qijun},
    title     = {Vision-and-Language Navigation via Causal Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13139-13150},
    doi={10.1109/CVPR52733.2024.01248},
    code={https://github.com/CrystalSixone/VLN-GOAT},
    gs_userid={AW2gZ8cAAAAJ},
    gs_paperid={W7OEmFMy1HYC},
    bibtex_show={true},
    abbr={GOAT},
    preview={wang2024vln.png},
    selected={true},
    abstract={In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-andlanguage navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.}
}

@InProceedings{ijcai2023p164,
  title     = {A Dual Semantic-Aware Recurrent Global-Adaptive Network for Vision-and-Language Navigation},
  author    = {Wang, Liuyi and He, Zongtao and Tang, Jiagui and Dang, Ronghao and Wang, Naijia and Liu, Chengju and Chen, Qijun},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {1479--1487},
  year      = {2023},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2023/164},
  url       = {https://doi.org/10.24963/ijcai.2023/164},
  code={https://github.com/CrystalSixone/DSRG},
  abstract={},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={Y0pCki6q_DkC},
  bibtex_show={true},
  abbr={DSRG},
  preview={wang2023dsrg.png},
  abstract={Vision-and-Language Navigation (VLN) is a realistic but challenging task that requires an agent to locate the target region using verbal and visual cues. While significant advancements have been achieved recently, there are still two broad limitations: (1) The explicit information mining for significant guiding semantics concealed in both vision and language is still under-explored; (2) The previously structured map method provides the average historical appearance of visited nodes, while it ignores distinctive contributions of various images and potent information retention in the reasoning process. This work proposes a dual semantic-aware recurrent global-adaptive network (DSRG) to address the above problems. First, DSRG proposes an instruction-guidance linguistic module (IGL) and an appearance-semantics visual module (ASV) for boosting vision and language semantic learning respectively. For the memory mechanism, a global adaptive aggregation module (GAA) is devised for explicit panoramic observation fusion, and a recurrent memory fusion module (RMF) is introduced to supply implicit temporal hidden states. Extensive experimental results on the R2R and REVERIE datasets demonstrate that our method achieves better performance than existing methods. Code is available at https://github.com/CrystalSixone/DSRG.}
}

@InProceedings{wang2024enhanced,
  title={Enhanced Language-guided Robot Navigation with Panoramic Semantic Depth Perception and Cross-modal Fusion},
  author={Wang, Liuyi and Tang, Jiagui and He, Zongtao and Dang, Ronghao and Liu, Chengju and Chen, Qijun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7726--7733},
  year={2024},
  doi={10.1109/IROS58592.2024.10801563},
  organization={IEEE},
  code={https://github.com/CrystalSixone/SEAT},
  abstract={},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={WF5omc3nYNoC},
  bibtex_show={true},
  abbr={SEAT},
  preview={wang2024seat.png},
  abstract={Integrating visual observation with linguistic instruction holds significant promise for enhancing robot navigation across unstructured environments and enriches the human-robot interaction experience. However, while panoramic RGB views furnish robots with extensive environmental visuals, current methods significantly overlook crucial semantic and depth cues. This incomplete representation may lead to misinterpretation or inadequate execution of language instructions, thereby impeding navigation performance and adaptability. In this paper, we introduce SEAT, a semantic-depth aware cross-modal transformer model. Our approach incorporates an efficient panoramic multi-type visual encoder to capture comprehensive environmental details. To mitigate the rigidity of feature mapping stemming from the freezing of pre-training encoders, we propose a novel region query pre-training task. Additionally, we leverage an improved dual-scale cross-modal transformer to facilitate the integration of instructions, topological memory, and action prediction. Extensive experiments on three language-guided robot navigation datasets demonstrate the efficacy of our model, achieving competitive navigation success rates with fewer parameters and computational load. Furthermore, we validate SEAT’s effectiveness in real-world scenarios by deploying it on a mobile robot across various environments. The code is available at https://github.com/CrystalSixone/SEAT.}
}

@article{wang2023res,
  title={RES-StS: Referring Expression Speaker via Self-training with Scorer for Goal-Oriented Vision-Language Navigation},
  author={Wang, Liuyi and He, Zongtao and Dang, Ronghao and Chen, Huiyi and Liu, Chengju and Chen, Qijun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2023},
  doi={10.1109/TCSVT.2022.3233554},
  publisher={IEEE},
  abbr={RES-StS},
  preview={wang2023ressts.png},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={zYLM7Y9cAGgC},
  bibtex_show={true},
  abstract={It is a rather practical but difficult task to find a specified target object via autonomous exploration based on natural language descriptions in an unstructured environment. Since the human-annotated data is expensive to gather for the goal-oriented vision-language navigation (GVLN) task, the size of the standard dataset is inadequate, which has significantly limited the accuracy of previous techniques. In this work, we aim to improve the robustness and generalization of the navigator by dynamically providing high-quality pseudo-instructions using a proposed RES-StS paradigm. Specifically, we establish a referring expression speaker (RES) to predict descriptive instructions for the given path to the goal object. Based on an environment-andobject fusion (EOF) module, RES derives spatial representations from the input trajectories, which are subsequently encoded by a number of transformer layers. Additionally, given that the quality of the pseudo labels is important for data augmentation while the limited dataset may also hinder RES learning, we propose to equip RES with a more effective generation ability by using the self-training approach. A trajectory-instruction matching scorer (TIMS) network based on contrastive learning is proposed to selectively use rehearsal of prior knowledge. Finally, all network modules in the system are integrated by suggesting a multi-stage training strategy, allowing them to assist one another and thus enhance performance on the GVLN task. Experimental results demonstrate the effectiveness of our approach. Compared with the SOTA methods, our method improves SR, SPL, and RGS by 4.72%, 2.55%, and 3.45% respectively, on the REVERIE dataset, and 4.58%, 3.75% and 3.14% respectively, on the SOON dataset.}
}

@InProceedings{dang2022unbiased,
  title={Unbiased Directed Object Attention Graph for Object Navigation},
  author={Dang, Ronghao and Shi, Zhuofan and Wang, Liuyi and He, Zongtao and Liu, Chengju and Chen, Qijun},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={3617--3627},
  year={2022},
  doi={10.1145/3503161.3547852},
  code={https://github.com/Rh-Dang/DOA},
  abstract={Object navigation tasks require agents to locate specific objects in unknown environments based on visual information. Previously, graph convolutions were used to implicitly explore the relationships between objects. However, due to differences in visibility among objects, it is easy to generate biases in object attention. Thus, in this paper, we propose a directed object attention (DOA) graph to guide the agent in explicitly learning the attention relationships between objects, thereby reducing the object attention bias. In particular, we use the DOA graph to perform unbiased adaptive object attention (UAOA) on the object features and unbiased adaptive image attention (UAIA) on the raw images, respectively. To distinguish features in different branches, a concise adaptive branch energy distribution (ABED) method is proposed. We assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE), respectively},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={UeHWp8X0CEIC},
  bibtex_show={true},
  abbr={DOA},
  preview={dang2022unbiased.png}
}

@InProceedings{dang2023multiple,
  title={Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation},
  author={Dang, ronghao and Chen, lu and Wang, liuyi and He zongtao and Liu, Chengju and Chen, Qijun},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023},
  month={23--29 Jul},
  html={https://proceedings.mlr.press/v202/dang23a.html},
  code={https://github.com/izilu/MT},
  abstract={We propose a meta-ability decoupling (MAD) paradigm, which brings together various object navigation methods in an architecture system, allowing them to mutually enhance each other and evolve together. Based on the MAD paradigm, we design a multiple thinking (MT) model that leverages distinct thinking to abstract various meta-abilities. Our method decouples meta-abilities from three aspects: input, encoding, and reward while employing the multiple thinking collaboration (MTC) module to promote mutual cooperation between thinking. MAD introduces a novel qualitative and quantitative interpretability system for object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we demonstrate that our method outperforms state-of-the-art (SOTA) methods on both typical and zero-shot object navigation tasks.},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={Tyk-4Ss8FVUC},
  bibtex_show={true},
  abbr={MAT},
  preview={dang2023mat.jpg},
  selected={true}
}

@inproceedings{dang2023search,
  title={Search for or Navigate to? Dual Adaptive Thinking for Object Navigation},
  author={Dang, Ronghao and Wang, Liuyi and He, Zongtao and Su, Shuai and Liu, Chengju and Chen, Qijun},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2023},
  doi={10.1109/iccv51070.2023.00758},
  code={https://github.com/Rh-Dang/DAT},
  abstract={"Search for" or "Navigate to"? When we find a specific object in an unknown environment, the two choices always arise in our subconscious mind. Before we see the target, we search for the target based on prior experience. Once we have seen the target, we can navigate to it by remembering the target location. However, recent object navigation methods consider using object association mostly to enhance the "search for" phase while neglecting the importance of the "navigate to" phase. Therefore, this paper proposes a dual adaptive thinking (DAT) method that flexibly adjusts thinking strategies in different navigation stages. Dual thinking includes both search thinking according to the object association ability and navigation thinking according to the target location ability. To make navigation thinking more effective, we design a target-oriented memory graph (TOMG) (which stores historical target information) and a target-aware multi-scale aggregator (TAMSA) (which encodes the relative position of the target). We assess our methods based on the AI2-Thor and RoboTHOR datasets. Compared with state-of-the-art (SOTA) methods, our approach significantly raises the overall success rate (SR) and success weighted by path length (SPL) while enhancing the agent’s performance in the "navigate to" phase.},
  gs_userid={AW2gZ8cAAAAJ},
  gs_paperid={9yKSN-GCB0IC},
  bibtex_show={true},
  abbr={DAT},
  preview={dang2023dat.png}
}
